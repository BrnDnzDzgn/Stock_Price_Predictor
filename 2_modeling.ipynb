{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9ea96b4c",
   "metadata": {},
   "source": [
    "## Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c7c5bd56",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models, callbacks, optimizers\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Load preprocessed data\n",
    "train_df = pd.read_pickle(\"train_df.pkl\")\n",
    "test_df  = pd.read_pickle(\"test_df.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "283ed421",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build sequences\n",
    "\n",
    "features = [\"Open\", \"High\", \"Low\", \"Close\", \"Volume\"]\n",
    "\n",
    "def make_sequences(df, window=30, horizon=1):\n",
    "    \"\"\"\n",
    "    Converts time-series data into sequences for an RNN.\n",
    "\n",
    "    window  = how many past days the model sees (30)\n",
    "    horizon = how many days into the future we predict (1, 5, 10)\n",
    "    \"\"\"\n",
    "    X, y = [], []\n",
    "\n",
    "    for stock in df[\"Stock\"].unique():\n",
    "        s = df[df[\"Stock\"] == stock]\n",
    "\n",
    "        data = s[features].values\n",
    "        target = s[\"Return\"].values\n",
    "\n",
    "        for i in range(window, len(s) - horizon):\n",
    "            X.append(data[i-window:i])\n",
    "            y.append(target[i+horizon])\n",
    "\n",
    "    return np.array(X), np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b03a6b7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(144501, 30, 5) (144501,)\n",
      "(640, 30, 5) (640,)\n"
     ]
    }
   ],
   "source": [
    "WINDOW = 30\n",
    "HORIZON = 1\n",
    "\n",
    "X_train, y_train = make_sequences(train_df, WINDOW, HORIZON)\n",
    "X_test, y_test   = make_sequences(test_df, WINDOW, HORIZON)\n",
    "\n",
    "print(X_train.shape, y_train.shape)\n",
    "print(X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d5ed783f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_train min/max/mean/std: -0.9999960726058523 0.9999999999999998 -0.0004396380700705552 0.05328160435645003\n",
      "y_test min/max/mean/std: -0.21628768829142098 0.38259110614382474 -0.005713482265587203 0.06286707668193339\n"
     ]
    }
   ],
   "source": [
    "print(\"y_train min/max/mean/std:\",\n",
    "      y_train.min(), y_train.max(), y_train.mean(), y_train.std())\n",
    "print(\"y_test min/max/mean/std:\",\n",
    "      y_test.min(), y_test.max(), y_test.mean(), y_test.std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9e181487",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train NaN: False inf: False\n",
      "y_train NaN: False inf: False\n",
      "X_test NaN: False inf: False\n",
      "y_test NaN: False inf: False\n"
     ]
    }
   ],
   "source": [
    "print(\"X_train NaN:\", np.isnan(X_train).any(), \"inf:\", np.isinf(X_train).any())\n",
    "print(\"y_train NaN:\", np.isnan(y_train).any(), \"inf:\", np.isinf(y_train).any())\n",
    "print(\"X_test NaN:\", np.isnan(X_test).any(), \"inf:\", np.isinf(X_test).any())\n",
    "print(\"y_test NaN:\", np.isnan(y_test).any(), \"inf:\", np.isinf(y_test).any())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "644bedc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "\u001b[1m509/509\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 18ms/step - loss: 0.0038 - mae: 0.0263 - val_loss: 8.3147e-04 - val_mae: 0.0190\n",
      "Epoch 2/30\n",
      "\u001b[1m509/509\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 18ms/step - loss: 0.0031 - mae: 0.0222 - val_loss: 8.3110e-04 - val_mae: 0.0190\n",
      "Epoch 3/30\n",
      "\u001b[1m509/509\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 18ms/step - loss: 0.0031 - mae: 0.0221 - val_loss: 8.3110e-04 - val_mae: 0.0190\n",
      "Epoch 4/30\n",
      "\u001b[1m509/509\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 18ms/step - loss: 0.0031 - mae: 0.0221 - val_loss: 8.3118e-04 - val_mae: 0.0190\n",
      "Epoch 5/30\n",
      "\u001b[1m509/509\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 18ms/step - loss: 0.0031 - mae: 0.0221 - val_loss: 8.3115e-04 - val_mae: 0.0190\n",
      "Epoch 6/30\n",
      "\u001b[1m509/509\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 18ms/step - loss: 0.0031 - mae: 0.0221 - val_loss: 8.3125e-04 - val_mae: 0.0190\n",
      "Epoch 7/30\n",
      "\u001b[1m509/509\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 18ms/step - loss: 0.0031 - mae: 0.0221 - val_loss: 8.3070e-04 - val_mae: 0.0190\n",
      "Epoch 8/30\n",
      "\u001b[1m509/509\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 18ms/step - loss: 0.0031 - mae: 0.0221 - val_loss: 8.3080e-04 - val_mae: 0.0190\n",
      "Epoch 9/30\n",
      "\u001b[1m509/509\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 18ms/step - loss: 0.0031 - mae: 0.0221 - val_loss: 8.3117e-04 - val_mae: 0.0190\n",
      "Epoch 10/30\n",
      "\u001b[1m509/509\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 18ms/step - loss: 0.0031 - mae: 0.0221 - val_loss: 8.3059e-04 - val_mae: 0.0190\n",
      "Epoch 11/30\n",
      "\u001b[1m509/509\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 18ms/step - loss: 0.0031 - mae: 0.0221 - val_loss: 8.3051e-04 - val_mae: 0.0190\n",
      "Epoch 12/30\n",
      "\u001b[1m509/509\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 18ms/step - loss: 0.0031 - mae: 0.0221 - val_loss: 8.3083e-04 - val_mae: 0.0190\n",
      "Epoch 13/30\n",
      "\u001b[1m509/509\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 18ms/step - loss: 0.0031 - mae: 0.0221 - val_loss: 8.3054e-04 - val_mae: 0.0190\n",
      "Epoch 14/30\n",
      "\u001b[1m509/509\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 18ms/step - loss: 0.0031 - mae: 0.0221 - val_loss: 8.3089e-04 - val_mae: 0.0190\n",
      "Epoch 15/30\n",
      "\u001b[1m509/509\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 18ms/step - loss: 0.0031 - mae: 0.0221 - val_loss: 8.3151e-04 - val_mae: 0.0191\n",
      "Epoch 16/30\n",
      "\u001b[1m509/509\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 18ms/step - loss: 0.0031 - mae: 0.0221 - val_loss: 8.3113e-04 - val_mae: 0.0190\n",
      "Test MSE: 0.003982838708907366\n",
      "Test MAE: 0.04758906736969948\n"
     ]
    }
   ],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "\n",
    "# Fixes randomness so results are reproducible\n",
    "# (same initialization, same training behavior)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# Number of input features per day (Open, High, Low, Close, Volume)\n",
    "n_features = X_train.shape[2]\n",
    "\n",
    "# Define a sequential neural network\n",
    "model = models.Sequential([\n",
    "\n",
    "    # Input shape:\n",
    "    # WINDOW = 30 days\n",
    "    # n_features = 5 features per day\n",
    "    # So each sample is a 30x5 matrix\n",
    "    layers.Input(shape=(WINDOW, n_features)),\n",
    "\n",
    "    # First LSTM layer\n",
    "    # 64 = number of memory units\n",
    "    # return_sequences=True means:\n",
    "    #   output a sequence of hidden states (one per day)\n",
    "    #   so that the next LSTM can process them\n",
    "    layers.LSTM(64, return_sequences=True),\n",
    "\n",
    "    # Dropout randomly removes 30% of neurons during training\n",
    "    # This prevents overfitting\n",
    "    layers.Dropout(0.3),\n",
    "\n",
    "    # Second LSTM layer\n",
    "    # This one compresses the 30-day sequence into one vector\n",
    "    # that summarizes recent market behavior\n",
    "    layers.LSTM(32),\n",
    "\n",
    "    # More dropout for regularization\n",
    "    layers.Dropout(0.3),\n",
    "\n",
    "    # A small dense (fully-connected) layer\n",
    "    # This learns nonlinear combinations of the LSTM output\n",
    "    layers.Dense(16, activation=\"relu\"),\n",
    "\n",
    "    layers.Dropout(0.2),\n",
    "\n",
    "    # Output layer:\n",
    "    # One number = predicted future return\n",
    "    layers.Dense(1)\n",
    "])\n",
    "\n",
    "# Compile the model:\n",
    "# Adam optimizer = efficient gradient descent\n",
    "# MSE loss = regression loss for predicting numbers\n",
    "model.compile(\n",
    "    optimizer=optimizers.Adam(learning_rate=1e-3),\n",
    "    loss=\"mse\",\n",
    "    metrics=[tf.keras.metrics.MeanAbsoluteError(name=\"mae\")]\n",
    ")\n",
    "\n",
    "# Stop training when validation error stops improving\n",
    "early_stop = callbacks.EarlyStopping(\n",
    "    monitor=\"val_loss\",\n",
    "    patience=5,\n",
    "    restore_best_weights=True\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_split=0.1,   # 10% of training data for validation\n",
    "    epochs=30,\n",
    "    batch_size=256,\n",
    "    callbacks=[early_stop],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Test the model on future data\n",
    "test_loss, test_mae = model.evaluate(X_test, y_test, verbose=0)\n",
    "\n",
    "print(\"Test MSE:\", test_loss)\n",
    "print(\"Test MAE:\", test_mae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd1b5d8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "\u001b[1m509/509\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 19ms/step - loss: 0.0032 - mae: 0.0251 - val_loss: 8.3121e-04 - val_mae: 0.0190\n",
      "Epoch 2/30\n",
      "\u001b[1m509/509\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 19ms/step - loss: 0.0029 - mae: 0.0224 - val_loss: 8.3093e-04 - val_mae: 0.0190\n",
      "Epoch 3/30\n",
      "\u001b[1m509/509\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 19ms/step - loss: 0.0028 - mae: 0.0222 - val_loss: 8.3096e-04 - val_mae: 0.0190\n",
      "Epoch 4/30\n",
      "\u001b[1m509/509\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 19ms/step - loss: 0.0028 - mae: 0.0221 - val_loss: 8.3100e-04 - val_mae: 0.0190\n",
      "Epoch 5/30\n",
      "\u001b[1m509/509\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 19ms/step - loss: 0.0028 - mae: 0.0221 - val_loss: 8.3117e-04 - val_mae: 0.0190\n",
      "Epoch 6/30\n",
      "\u001b[1m509/509\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 19ms/step - loss: 0.0028 - mae: 0.0221 - val_loss: 8.3122e-04 - val_mae: 0.0190\n",
      "Epoch 7/30\n",
      "\u001b[1m509/509\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 18ms/step - loss: 0.0028 - mae: 0.0221 - val_loss: 8.3097e-04 - val_mae: 0.0190\n",
      "Epoch 1/30\n",
      "\u001b[1m508/508\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 19ms/step - loss: 0.0036 - mae: 0.0256 - val_loss: 8.3125e-04 - val_mae: 0.0190\n",
      "Epoch 2/30\n",
      "\u001b[1m508/508\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 18ms/step - loss: 0.0031 - mae: 0.0223 - val_loss: 8.3248e-04 - val_mae: 0.0190\n",
      "Epoch 3/30\n",
      "\u001b[1m508/508\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 19ms/step - loss: 0.0029 - mae: 0.0223 - val_loss: 8.3229e-04 - val_mae: 0.0190\n",
      "Epoch 4/30\n",
      "\u001b[1m508/508\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 18ms/step - loss: 0.0028 - mae: 0.0223 - val_loss: 8.3257e-04 - val_mae: 0.0190\n",
      "Epoch 5/30\n",
      "\u001b[1m508/508\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 18ms/step - loss: 0.0028 - mae: 0.0222 - val_loss: 8.3255e-04 - val_mae: 0.0190\n",
      "Epoch 6/30\n",
      "\u001b[1m508/508\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 18ms/step - loss: 0.0028 - mae: 0.0222 - val_loss: 8.3275e-04 - val_mae: 0.0190\n",
      "Epoch 1/30\n",
      "\u001b[1m508/508\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 19ms/step - loss: 0.0034 - mae: 0.0251 - val_loss: 8.3384e-04 - val_mae: 0.0190\n",
      "Epoch 2/30\n",
      "\u001b[1m508/508\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 18ms/step - loss: 0.0030 - mae: 0.0223 - val_loss: 8.3228e-04 - val_mae: 0.0190\n",
      "Epoch 3/30\n",
      "\u001b[1m508/508\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 19ms/step - loss: 0.0029 - mae: 0.0224 - val_loss: 8.3257e-04 - val_mae: 0.0190\n",
      "Epoch 4/30\n",
      "\u001b[1m508/508\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 19ms/step - loss: 0.0029 - mae: 0.0222 - val_loss: 8.3223e-04 - val_mae: 0.0190\n",
      "Epoch 5/30\n",
      "\u001b[1m508/508\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 19ms/step - loss: 0.0028 - mae: 0.0222 - val_loss: 8.3223e-04 - val_mae: 0.0190\n",
      "Epoch 6/30\n",
      "\u001b[1m508/508\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 19ms/step - loss: 0.0028 - mae: 0.0222 - val_loss: 8.3219e-04 - val_mae: 0.0190\n",
      "Epoch 7/30\n",
      "\u001b[1m508/508\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 19ms/step - loss: 0.0028 - mae: 0.0221 - val_loss: 8.3217e-04 - val_mae: 0.0190\n",
      "Epoch 8/30\n",
      "\u001b[1m508/508\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 18ms/step - loss: 0.0028 - mae: 0.0221 - val_loss: 8.3216e-04 - val_mae: 0.0190\n",
      "Epoch 9/30\n",
      "\u001b[1m508/508\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 18ms/step - loss: 0.0028 - mae: 0.0222 - val_loss: 8.3216e-04 - val_mae: 0.0190\n",
      "Epoch 10/30\n",
      "\u001b[1m508/508\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 18ms/step - loss: 0.0028 - mae: 0.0221 - val_loss: 8.3216e-04 - val_mae: 0.0190\n",
      "Epoch 11/30\n",
      "\u001b[1m508/508\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 18ms/step - loss: 0.0028 - mae: 0.0221 - val_loss: 8.3216e-04 - val_mae: 0.0190\n",
      "Epoch 12/30\n",
      "\u001b[1m508/508\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 18ms/step - loss: 0.0028 - mae: 0.0221 - val_loss: 8.3216e-04 - val_mae: 0.0190\n",
      "Epoch 13/30\n",
      "\u001b[1m508/508\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 19ms/step - loss: 0.0028 - mae: 0.0221 - val_loss: 8.3216e-04 - val_mae: 0.0190\n",
      "Epoch 14/30\n",
      "\u001b[1m508/508\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 19ms/step - loss: 0.0028 - mae: 0.0221 - val_loss: 8.3216e-04 - val_mae: 0.0190\n",
      "Epoch 15/30\n",
      "\u001b[1m508/508\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 19ms/step - loss: 0.0028 - mae: 0.0221 - val_loss: 8.3216e-04 - val_mae: 0.0190\n",
      "Epoch 16/30\n",
      "\u001b[1m508/508\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 19ms/step - loss: 0.0028 - mae: 0.0221 - val_loss: 8.3216e-04 - val_mae: 0.0190\n",
      "Epoch 17/30\n",
      "\u001b[1m508/508\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 19ms/step - loss: 0.0028 - mae: 0.0221 - val_loss: 8.3216e-04 - val_mae: 0.0190\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MSE</th>\n",
       "      <th>MAE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Horizon_1</th>\n",
       "      <td>0.003991</td>\n",
       "      <td>0.047703</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Horizon_5</th>\n",
       "      <td>0.004479</td>\n",
       "      <td>0.051994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Horizon_10</th>\n",
       "      <td>0.005132</td>\n",
       "      <td>0.056280</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 MSE       MAE\n",
       "Horizon_1   0.003991  0.047703\n",
       "Horizon_5   0.004479  0.051994\n",
       "Horizon_10  0.005132  0.056280"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Different Prediction Horizons\n",
    "\n",
    "def run_experiment(horizon):\n",
    "    tf.keras.backend.clear_session()\n",
    "    Xtr, ytr = make_sequences(train_df, WINDOW, horizon)\n",
    "    Xte, yte = make_sequences(test_df, WINDOW, horizon)\n",
    "\n",
    "    tf.random.set_seed(42)\n",
    "\n",
    "    model = models.Sequential([\n",
    "        layers.Input(shape=(WINDOW, Xtr.shape[2])),\n",
    "        layers.LSTM(64, return_sequences=True),\n",
    "        layers.Dropout(0.3),\n",
    "        layers.LSTM(32),\n",
    "        layers.Dropout(0.3),\n",
    "        layers.Dense(16, activation=\"relu\"),\n",
    "        layers.Dense(1)\n",
    "    ])\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=optimizers.Adam(1e-3),\n",
    "        loss=\"mse\",\n",
    "        metrics=[tf.keras.metrics.MeanAbsoluteError(name=\"mae\")]\n",
    "    )\n",
    "\n",
    "    # Stop training when validation error stops improving\n",
    "    early_stop = callbacks.EarlyStopping(\n",
    "        monitor=\"val_loss\",\n",
    "        patience=5,\n",
    "        restore_best_weights=True\n",
    "    )\n",
    "\n",
    "    model.fit(\n",
    "        Xtr, ytr,\n",
    "        validation_split=0.1,\n",
    "        epochs=30,\n",
    "        batch_size=256,\n",
    "        callbacks=[early_stop],\n",
    "        verbose=1\n",
    "    )\n",
    "\n",
    "    mse, mae = model.evaluate(Xte, yte, verbose=0)\n",
    "    return mse, mae\n",
    "\n",
    "results = {}\n",
    "\n",
    "for h in [1, 5, 10]:\n",
    "    mse, mae = run_experiment(h)\n",
    "    results[f\"Horizon_{h}\"] = {\"MSE\": mse, \"MAE\": mae}\n",
    "\n",
    "pd.DataFrame(results).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c6c22899",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 20ms/step - loss: 0.0018 - mae: 0.0279 - val_loss: 6.2207e-04 - val_mae: 0.0165\n",
      "Epoch 2/30\n",
      "\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 19ms/step - loss: 0.0011 - mae: 0.0228 - val_loss: 6.2122e-04 - val_mae: 0.0165\n",
      "Epoch 3/30\n",
      "\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 19ms/step - loss: 0.0011 - mae: 0.0226 - val_loss: 6.2089e-04 - val_mae: 0.0165\n",
      "Epoch 4/30\n",
      "\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 19ms/step - loss: 0.0011 - mae: 0.0225 - val_loss: 6.2092e-04 - val_mae: 0.0165\n",
      "Epoch 5/30\n",
      "\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 19ms/step - loss: 0.0011 - mae: 0.0225 - val_loss: 6.2082e-04 - val_mae: 0.0165\n",
      "Epoch 6/30\n",
      "\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 19ms/step - loss: 0.0011 - mae: 0.0225 - val_loss: 6.2080e-04 - val_mae: 0.0165\n",
      "Epoch 7/30\n",
      "\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 19ms/step - loss: 0.0011 - mae: 0.0225 - val_loss: 6.2078e-04 - val_mae: 0.0165\n",
      "Epoch 8/30\n",
      "\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 19ms/step - loss: 0.0011 - mae: 0.0225 - val_loss: 6.2082e-04 - val_mae: 0.0165\n",
      "Epoch 9/30\n",
      "\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 19ms/step - loss: 0.0011 - mae: 0.0224 - val_loss: 6.2075e-04 - val_mae: 0.0165\n",
      "Epoch 10/30\n",
      "\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 19ms/step - loss: 0.0011 - mae: 0.0224 - val_loss: 6.2083e-04 - val_mae: 0.0165\n",
      "Epoch 11/30\n",
      "\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 19ms/step - loss: 0.0011 - mae: 0.0224 - val_loss: 6.2086e-04 - val_mae: 0.0165\n",
      "Epoch 12/30\n",
      "\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 19ms/step - loss: 0.0011 - mae: 0.0224 - val_loss: 6.2084e-04 - val_mae: 0.0165\n",
      "Epoch 13/30\n",
      "\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 19ms/step - loss: 0.0011 - mae: 0.0224 - val_loss: 6.2090e-04 - val_mae: 0.0165\n",
      "Epoch 14/30\n",
      "\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 19ms/step - loss: 0.0011 - mae: 0.0224 - val_loss: 6.2083e-04 - val_mae: 0.0165\n",
      "Epoch 1/30\n",
      "\u001b[1m286/286\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 19ms/step - loss: 0.0012 - mae: 0.0230 - val_loss: 5.7103e-04 - val_mae: 0.0165\n",
      "Epoch 2/30\n",
      "\u001b[1m286/286\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 19ms/step - loss: 8.9840e-04 - mae: 0.0202 - val_loss: 5.7028e-04 - val_mae: 0.0165\n",
      "Epoch 3/30\n",
      "\u001b[1m286/286\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 19ms/step - loss: 8.9443e-04 - mae: 0.0201 - val_loss: 5.7023e-04 - val_mae: 0.0165\n",
      "Epoch 4/30\n",
      "\u001b[1m286/286\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 19ms/step - loss: 8.9400e-04 - mae: 0.0201 - val_loss: 5.7024e-04 - val_mae: 0.0165\n",
      "Epoch 5/30\n",
      "\u001b[1m286/286\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 19ms/step - loss: 8.9372e-04 - mae: 0.0201 - val_loss: 5.7023e-04 - val_mae: 0.0165\n",
      "Epoch 6/30\n",
      "\u001b[1m286/286\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 19ms/step - loss: 8.9313e-04 - mae: 0.0201 - val_loss: 5.7031e-04 - val_mae: 0.0165\n",
      "Epoch 7/30\n",
      "\u001b[1m286/286\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 19ms/step - loss: 8.9334e-04 - mae: 0.0201 - val_loss: 5.7049e-04 - val_mae: 0.0165\n",
      "Epoch 8/30\n",
      "\u001b[1m286/286\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 19ms/step - loss: 8.9335e-04 - mae: 0.0201 - val_loss: 5.7061e-04 - val_mae: 0.0165\n",
      "Epoch 1/30\n",
      "\u001b[1m509/509\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 19ms/step - loss: 0.0033 - mae: 0.0249 - val_loss: 8.3158e-04 - val_mae: 0.0190\n",
      "Epoch 2/30\n",
      "\u001b[1m509/509\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 19ms/step - loss: 0.0029 - mae: 0.0224 - val_loss: 8.3112e-04 - val_mae: 0.0190\n",
      "Epoch 3/30\n",
      "\u001b[1m509/509\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 19ms/step - loss: 0.0028 - mae: 0.0222 - val_loss: 8.3126e-04 - val_mae: 0.0190\n",
      "Epoch 4/30\n",
      "\u001b[1m509/509\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 19ms/step - loss: 0.0028 - mae: 0.0222 - val_loss: 8.3134e-04 - val_mae: 0.0190\n",
      "Epoch 5/30\n",
      "\u001b[1m509/509\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 19ms/step - loss: 0.0028 - mae: 0.0222 - val_loss: 8.3099e-04 - val_mae: 0.0190\n",
      "Epoch 6/30\n",
      "\u001b[1m509/509\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 18ms/step - loss: 0.0028 - mae: 0.0221 - val_loss: 8.3141e-04 - val_mae: 0.0190\n",
      "Epoch 7/30\n",
      "\u001b[1m509/509\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 19ms/step - loss: 0.0028 - mae: 0.0221 - val_loss: 8.3101e-04 - val_mae: 0.0190\n",
      "Epoch 8/30\n",
      "\u001b[1m509/509\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 19ms/step - loss: 0.0028 - mae: 0.0221 - val_loss: 8.3130e-04 - val_mae: 0.0190\n",
      "Epoch 9/30\n",
      "\u001b[1m509/509\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 19ms/step - loss: 0.0028 - mae: 0.0221 - val_loss: 8.3115e-04 - val_mae: 0.0190\n",
      "Epoch 10/30\n",
      "\u001b[1m509/509\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 18ms/step - loss: 0.0028 - mae: 0.0221 - val_loss: 8.3102e-04 - val_mae: 0.0190\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'5_stocks': [0.003379339352250099, 0.04530029371380806],\n",
       " '10_stocks': [0.003212945070117712, 0.044040340930223465],\n",
       " '20_stocks': [0.004214080050587654, 0.049018822610378265]}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Different Number of Stocks\n",
    "\n",
    "def subset_stocks(df, n):\n",
    "    stocks = df[\"Stock\"].unique()[:n]\n",
    "    return df[df[\"Stock\"].isin(stocks)]\n",
    "\n",
    "stock_results = {}\n",
    "\n",
    "for n in [5, 10, 20]:\n",
    "    tf.keras.backend.clear_session()\n",
    "    \n",
    "    tr = subset_stocks(train_df, n)\n",
    "    te = subset_stocks(test_df, n)\n",
    "\n",
    "    Xtr, ytr = make_sequences(tr, WINDOW, 1)\n",
    "    Xte, yte = make_sequences(te, WINDOW, 1)\n",
    "\n",
    "    tf.random.set_seed(42)\n",
    "\n",
    "    model = models.Sequential([\n",
    "        layers.Input(shape=(WINDOW, Xtr.shape[2])),\n",
    "        layers.LSTM(64, return_sequences=True),\n",
    "        layers.Dropout(0.3),\n",
    "        layers.LSTM(32),\n",
    "        layers.Dropout(0.3),\n",
    "        layers.Dense(16, activation=\"relu\"),\n",
    "        layers.Dense(1)\n",
    "    ])\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=optimizers.Adam(learning_rate=1e-3),\n",
    "        loss=\"mse\",\n",
    "        metrics=[tf.keras.metrics.MeanAbsoluteError(name=\"mae\")]\n",
    "    )\n",
    "\n",
    "    # Stop training when validation error stops improving\n",
    "    early_stop = callbacks.EarlyStopping(\n",
    "        monitor=\"val_loss\",\n",
    "        patience=5,\n",
    "        restore_best_weights=True\n",
    "    )\n",
    "\n",
    "    model.fit(\n",
    "        Xtr, ytr,\n",
    "        validation_split=0.1,\n",
    "        epochs=30,\n",
    "        batch_size=256,\n",
    "        callbacks=[early_stop],\n",
    "        verbose=1\n",
    "    )\n",
    "\n",
    "    mse = model.evaluate(Xte, yte, verbose=0)\n",
    "\n",
    "    stock_results[f\"{n}_stocks\"] = mse\n",
    "\n",
    "stock_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9301c8f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 20ms/step - loss: 0.0032 - mae: 0.0338 - val_loss: 6.7490e-04 - val_mae: 0.0176\n",
      "Epoch 2/30\n",
      "\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 18ms/step - loss: 0.0012 - mae: 0.0235 - val_loss: 6.2417e-04 - val_mae: 0.0165\n",
      "Epoch 3/30\n",
      "\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 19ms/step - loss: 0.0011 - mae: 0.0230 - val_loss: 6.2282e-04 - val_mae: 0.0164\n",
      "Epoch 4/30\n",
      "\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 19ms/step - loss: 0.0011 - mae: 0.0227 - val_loss: 6.2214e-04 - val_mae: 0.0164\n",
      "Epoch 5/30\n",
      "\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 19ms/step - loss: 0.0011 - mae: 0.0226 - val_loss: 6.2108e-04 - val_mae: 0.0164\n",
      "Epoch 6/30\n",
      "\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 19ms/step - loss: 0.0011 - mae: 0.0225 - val_loss: 6.2097e-04 - val_mae: 0.0164\n",
      "Epoch 7/30\n",
      "\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 19ms/step - loss: 0.0011 - mae: 0.0225 - val_loss: 6.2086e-04 - val_mae: 0.0165\n",
      "Epoch 8/30\n",
      "\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 19ms/step - loss: 0.0011 - mae: 0.0225 - val_loss: 6.2084e-04 - val_mae: 0.0165\n",
      "Epoch 9/30\n",
      "\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 19ms/step - loss: 0.0011 - mae: 0.0225 - val_loss: 6.2084e-04 - val_mae: 0.0165\n",
      "Epoch 10/30\n",
      "\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 18ms/step - loss: 0.0011 - mae: 0.0224 - val_loss: 6.2084e-04 - val_mae: 0.0165\n",
      "Epoch 11/30\n",
      "\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 19ms/step - loss: 0.0011 - mae: 0.0224 - val_loss: 6.2085e-04 - val_mae: 0.0165\n",
      "Epoch 12/30\n",
      "\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 19ms/step - loss: 0.0011 - mae: 0.0224 - val_loss: 6.2085e-04 - val_mae: 0.0165\n",
      "Epoch 13/30\n",
      "\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 19ms/step - loss: 0.0011 - mae: 0.0224 - val_loss: 6.2086e-04 - val_mae: 0.0165\n",
      "Epoch 1/30\n",
      "\u001b[1m286/286\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 19ms/step - loss: 0.0013 - mae: 0.0237 - val_loss: 5.7118e-04 - val_mae: 0.0165\n",
      "Epoch 2/30\n",
      "\u001b[1m286/286\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 19ms/step - loss: 9.0066e-04 - mae: 0.0202 - val_loss: 5.7023e-04 - val_mae: 0.0165\n",
      "Epoch 3/30\n",
      "\u001b[1m286/286\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 19ms/step - loss: 8.9511e-04 - mae: 0.0201 - val_loss: 5.7030e-04 - val_mae: 0.0165\n",
      "Epoch 4/30\n",
      "\u001b[1m286/286\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 19ms/step - loss: 8.9400e-04 - mae: 0.0201 - val_loss: 5.7050e-04 - val_mae: 0.0165\n",
      "Epoch 5/30\n",
      "\u001b[1m286/286\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 18ms/step - loss: 8.9330e-04 - mae: 0.0201 - val_loss: 5.7081e-04 - val_mae: 0.0165\n",
      "Epoch 6/30\n",
      "\u001b[1m286/286\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 18ms/step - loss: 8.9344e-04 - mae: 0.0201 - val_loss: 5.7056e-04 - val_mae: 0.0165\n",
      "Epoch 7/30\n",
      "\u001b[1m286/286\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 19ms/step - loss: 8.9274e-04 - mae: 0.0201 - val_loss: 5.7089e-04 - val_mae: 0.0165\n",
      "Epoch 1/30\n",
      "\u001b[1m508/508\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 19ms/step - loss: 0.0033 - mae: 0.0242 - val_loss: 8.3326e-04 - val_mae: 0.0190\n",
      "Epoch 2/30\n",
      "\u001b[1m508/508\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 19ms/step - loss: 0.0031 - mae: 0.0222 - val_loss: 8.3248e-04 - val_mae: 0.0190\n",
      "Epoch 3/30\n",
      "\u001b[1m508/508\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 18ms/step - loss: 0.0031 - mae: 0.0221 - val_loss: 8.3227e-04 - val_mae: 0.0190\n",
      "Epoch 4/30\n",
      "\u001b[1m508/508\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 19ms/step - loss: 0.0031 - mae: 0.0221 - val_loss: 8.3221e-04 - val_mae: 0.0190\n",
      "Epoch 5/30\n",
      "\u001b[1m508/508\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 19ms/step - loss: 0.0031 - mae: 0.0221 - val_loss: 8.3217e-04 - val_mae: 0.0190\n",
      "Epoch 6/30\n",
      "\u001b[1m508/508\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 19ms/step - loss: 0.0031 - mae: 0.0221 - val_loss: 8.3182e-04 - val_mae: 0.0190\n",
      "Epoch 7/30\n",
      "\u001b[1m508/508\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 19ms/step - loss: 0.0031 - mae: 0.0221 - val_loss: 8.3333e-04 - val_mae: 0.0191\n",
      "Epoch 8/30\n",
      "\u001b[1m508/508\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 19ms/step - loss: 0.0031 - mae: 0.0221 - val_loss: 8.3405e-04 - val_mae: 0.0191\n",
      "Epoch 9/30\n",
      "\u001b[1m508/508\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 18ms/step - loss: 0.0031 - mae: 0.0221 - val_loss: 8.3331e-04 - val_mae: 0.0191\n",
      "Epoch 10/30\n",
      "\u001b[1m508/508\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 19ms/step - loss: 0.0031 - mae: 0.0221 - val_loss: 8.3374e-04 - val_mae: 0.0191\n",
      "Epoch 11/30\n",
      "\u001b[1m508/508\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 19ms/step - loss: 0.0031 - mae: 0.0221 - val_loss: 8.3612e-04 - val_mae: 0.0191\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Stocks</th>\n",
       "      <th>Horizon</th>\n",
       "      <th>MSE</th>\n",
       "      <th>MAE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0.003386</td>\n",
       "      <td>0.045362</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10</td>\n",
       "      <td>5</td>\n",
       "      <td>0.003617</td>\n",
       "      <td>0.048338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20</td>\n",
       "      <td>10</td>\n",
       "      <td>0.005077</td>\n",
       "      <td>0.056044</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Stocks  Horizon       MSE       MAE\n",
       "0       5        1  0.003386  0.045362\n",
       "1      10        5  0.003617  0.048338\n",
       "2      20       10  0.005077  0.056044"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "configs = [\n",
    "    (5, 1),\n",
    "    (10, 5),\n",
    "    (20, 10),\n",
    "]\n",
    "\n",
    "results = []\n",
    "\n",
    "for n_stocks, horizon in configs:\n",
    "    tr = subset_stocks(train_df, n_stocks)\n",
    "    te = subset_stocks(test_df, n_stocks)\n",
    "\n",
    "    Xtr, ytr = make_sequences(tr, WINDOW, horizon)\n",
    "    Xte, yte = make_sequences(te, WINDOW, horizon)\n",
    "\n",
    "    tf.keras.backend.clear_session()\n",
    "    tf.random.set_seed(42)\n",
    "\n",
    "    model = models.Sequential([\n",
    "        layers.Input(shape=(WINDOW, Xtr.shape[2])),\n",
    "        layers.LSTM(64, return_sequences=True),\n",
    "        layers.Dropout(0.3),\n",
    "        layers.LSTM(32),\n",
    "        layers.Dropout(0.3),\n",
    "        layers.Dense(16, activation=\"relu\"),\n",
    "        layers.Dense(1)\n",
    "    ])\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=optimizers.Adam(1e-3),\n",
    "        loss=\"mse\",\n",
    "        metrics=[tf.keras.metrics.MeanAbsoluteError(name=\"mae\")]\n",
    "    )\n",
    "\n",
    "    model.fit(\n",
    "        Xtr, ytr,\n",
    "        validation_split=0.1,\n",
    "        epochs=30,\n",
    "        batch_size=256,\n",
    "        callbacks=[callbacks.EarlyStopping(\n",
    "            monitor=\"val_loss\",\n",
    "            patience=5,\n",
    "            restore_best_weights=True\n",
    "        )],\n",
    "        verbose=1\n",
    "    )\n",
    "\n",
    "    mse, mae = model.evaluate(Xte, yte, verbose=0)\n",
    "\n",
    "    results.append({\n",
    "        \"Stocks\": n_stocks,\n",
    "        \"Horizon\": horizon,\n",
    "        \"MSE\": mse,\n",
    "        \"MAE\": mae\n",
    "    })\n",
    "\n",
    "pd.DataFrame(results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".stock_price",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
